{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j06pz3I2MGws"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook covers how to use tf.keras to build a classification model like what we talked about in the previous series.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbcPzAQAwIIi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "# CHANGE THESE TO FIT YOUR FOLDER NAMES\n",
    "base_folder = '##PUT YOUR BASE FOLDER HERE##'\n",
    "data_folder = os.path.join( base_folder, '##YOUR CATS AND DOGS FOLDER##' )\n",
    "save_folder = os.path.join( base_folder, '##WHERE YOU WANT TO SAVE YOUR MODELS##' )\n",
    "input_shape = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWGL20iRqm52"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvhoMX949-WP"
   },
   "outputs": [],
   "source": [
    "def basic_classification_model(input_shape, model_name='basic_model'):\n",
    "  inputs = keras.Input(shape=input_shape)\n",
    "  x = layers.Conv2D(32, 3, padding='same')(inputs)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  block_1_output = layers.MaxPooling2D(2)(x) # 112\n",
    "\n",
    "  x = layers.Conv2D(64, 3, padding='same')(block_1_output)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.add([x, block_1_output])\n",
    "  x = layers.Conv2D(128, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  block_2_output = layers.MaxPooling2D(2)(x) #56\n",
    "\n",
    "  x = layers.Conv2D(128, 3, padding='same')(block_2_output)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(128, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.add([x, block_2_output])\n",
    "  x = layers.Conv2D(256, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  block_3_output = layers.MaxPooling2D(2)(x) #28\n",
    "\n",
    "  x = layers.Conv2D(256, 3, padding='same')(block_3_output)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(256, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.add([x, block_3_output])\n",
    "  x = layers.Conv2D(512, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  block_4_output = layers.MaxPooling2D(2)(x) #14\n",
    "\n",
    "  x = layers.Conv2D(512, 3, padding='same')(block_4_output)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.Conv2D(512, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.add([x, block_4_output])\n",
    "  x = layers.Conv2D(1024, 3, padding='same')(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  x = layers.MaxPooling2D(2)(x) #7\n",
    "\n",
    "  x = layers.GlobalAveragePooling2D()(x) #1024\n",
    "\n",
    "  x = layers.Dense(1024)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "\n",
    "  x = layers.Dropout(0.5)(x) # Alternatively, we can use:\n",
    "  x = layers.Dense(2)(x) # x = layers.Dense(1)(x)\n",
    "  predictions = layers.Softmax()(x) # predictions = layers.Sigmoid()(x)\n",
    "\n",
    "  model = keras.Model(inputs, predictions, name=model_name)\n",
    "  model.compile( optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                 loss=keras.losses.CategoricalCrossentropy(from_logits=False), # BinaryCrossentropy instead\n",
    "                 metrics=['accuracy'] )\n",
    "  return model\n",
    "  \n",
    "  # If you want to know more about optimizers: https://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tcEIvrxKM75"
   },
   "outputs": [],
   "source": [
    "model_context = 'basic_model'\n",
    "model = basic_classification_model(input_shape, model_name=model_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgkEXhw9LeKv"
   },
   "outputs": [],
   "source": [
    "# To see what you've built, you can use model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-isWkF0KSkH"
   },
   "outputs": [],
   "source": [
    "# You can also view a graphical plot of the model\n",
    "model_plot = tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "display(model_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RT1Q3hl9LgBW"
   },
   "outputs": [],
   "source": [
    "# To train the model, we first need data. \n",
    "# Inside data_folder there are two folders: \"dog\" and \"cat\", which have images of dogs and cats respectively.\n",
    "# We will use keras's built-in utilities to read the images from these folders and feed them into our model for training.\n",
    "\n",
    "# But first, let's look at some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6B80-R7zTO6q"
   },
   "outputs": [],
   "source": [
    "cat_folder = os.path.join( data_folder, 'cat' )\n",
    "cat_files = list(os.listdir( cat_folder ))\n",
    "dog_folder = os.path.join( data_folder, 'dog' )\n",
    "dog_files = list(os.listdir( dog_folder ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNWUjlctUGjc"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Look at the first cat image\n",
    "Image(filename= os.path.join( cat_folder, cat_files[0] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSELoDidUJMz"
   },
   "outputs": [],
   "source": [
    "# Look at the last dog image\n",
    "Image(filename= os.path.join( dog_folder, dog_files[-1] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABAK24COUd1E"
   },
   "outputs": [],
   "source": [
    "# Set up the data generators to read from our data_folder\n",
    "bs = 32 # The batch size is 32\n",
    "\n",
    "# An object that applies transformations to the images before they are consumed by the model\n",
    "# These transformations include (1) preprocessing, like rescaling or normalization (2) data augmentation\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255, # divide each pixel value by 255. Each pixel is in the range 0-255, so after division it is in 0-1\n",
    "        rotation_range=20, # rotate the image between -20 to +20 degrees\n",
    "        width_shift_range=0.2, # translate the image left-right for 20% of the image's width\n",
    "        height_shift_range=0.2, # same, for up-down and height\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2)\n",
    "print('Making training data generator...')\n",
    "train_gen = datagen.flow_from_directory(\n",
    "        data_folder,\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=bs,\n",
    "        subset='training')\n",
    "print('Making validation data generator...')\n",
    "val_gen = datagen.flow_from_directory(\n",
    "        data_folder,\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=bs,\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjZSyYr-cWH8"
   },
   "outputs": [],
   "source": [
    "# Callbacks are useful to help you monitor the progress of the training as it is going on\n",
    "# and to intervene in between if certain conditions are met.\n",
    "\n",
    "# This monitors the validation loss of the model as it is training, saving a full copy of the model and it's specs everytime the\n",
    "# validation loss reaches an unprecedented low.\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join( save_folder, '{}-best_val_loss.h5'.format(model_context) ),\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "# If the validation loss doesn't improve for 20 epochs, stop training\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# If the validation loss doesn't improve for 5 epochs, reduce the learning rate to 0.2 times it's previous value\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-Tj_m1baFeq"
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "n_epochs=50\n",
    "model.fit(train_gen,\n",
    "          epochs=n_epochs,\n",
    "          steps_per_epoch=train_gen.n // bs,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_gen.n // bs,\n",
    "          callbacks=[model_checkpoint, earlystopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fejGP2YqfCPP"
   },
   "outputs": [],
   "source": [
    "# Let's load an independent image to try our model on\n",
    "# CHANGE TO YOUR FOLDER AND IMAGE NAMES\n",
    "test_img_path = os.path.join(base_folder, 'src_images', 'boss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNDYODixV0JD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def run_image_on_model(img_path, model, label_map):\n",
    "  pil_img = load_img(test_img_path)\n",
    "  pil_img = pil_img.resize( input_shape[:2] )\n",
    "  img_arr = img_to_array(pil_img)\n",
    "  # Remember to normalize the image values the same way you did when you trained the model\n",
    "  img_arr = img_arr / 255.\n",
    "  # We need to wrap this in an np.array with dimensions (b,H,W,C). Currently, the shape is only (H,W,C)\n",
    "  img_arr = np.array( [img_arr] )\n",
    "  pred = model.predict(img_arr, batch_size=1)[0]\n",
    "  pred_idx = np.argmax(pred)\n",
    "  return label_map[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbsCNrmSaSXB"
   },
   "outputs": [],
   "source": [
    "# The generator's internal labeling of cat/dog\n",
    "print(train_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QcEmV7yMeEIq"
   },
   "outputs": [],
   "source": [
    "# Construct a reverse mapping\n",
    "label_map = {v:k for k,v in train_gen.class_indices.items()}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETDFL6lXeS7u"
   },
   "outputs": [],
   "source": [
    "Image(filename=test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXlT2_aFe910"
   },
   "outputs": [],
   "source": [
    "# Your results may vary here. It's possible that your model will predict correctly\n",
    "print( 'model prediction: {}'.format(run_image_on_model(test_img_path, model, label_map)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-IWlqvwhz1O"
   },
   "outputs": [],
   "source": [
    "# Let's try and improve the accuracy of the model by applying transfer learning.\n",
    "# The key idea behind transfer learning is to leverage on a more powerful model trained on a different but related task.\n",
    "# In this case, we are going to use a pre-trained ResNet50 from the keras applications module, that was pre-trained on a 1000-category classification dataset\n",
    "# Both are classification tasks, so they are related. However, the pre-trained model predicts a probability vector between 1000 classes\n",
    "# So we'd need to modify it to only predict cat and dog classes.\n",
    "\n",
    "# The features extracted by the powerful model are better than what we have now. The idea is to use these features to \n",
    "# \"warm-up\" our own model.\n",
    "# Once the model is sufficiently warmed up, we can train both parts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqja549cjgOC"
   },
   "outputs": [],
   "source": [
    "# When include_top=False, we are discarding the 1000 category predictions\n",
    "transfer_model = tf.keras.applications.ResNet50(input_shape=input_shape, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoiDqvxIkEGl"
   },
   "outputs": [],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhXkymr7kftG"
   },
   "outputs": [],
   "source": [
    "# Notice that we are left with a 7x7 square of depth 2048.\n",
    "# We will apply GAP to reduce this tensor to a vector of length 2048, and train a classifier at the end to distinguish between two classes\n",
    "# But first, we should disable training for the ResNet50 temporarily:\n",
    "for layer in transfer_model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fQGUzZHlKP1"
   },
   "outputs": [],
   "source": [
    "# Inspect the model to see that the number of trainable params is zero\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFOu8EqylZMH"
   },
   "outputs": [],
   "source": [
    "# Now let's rig together a new model\n",
    "def transfer_learning_model(input_shape, base_model, model_name='transfer_learning_model'):\n",
    "  # Freeze the base model\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  inputs = keras.Input(shape=input_shape)\n",
    "  # First, run the input through the power model. x contains good extracted features.\n",
    "  x = base_model(inputs)\n",
    "  # Notice that the rest below are more or less the same\n",
    "  x = layers.GlobalAveragePooling2D()(x) #2048\n",
    "\n",
    "  x = layers.Dense(1024)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  x = layers.ReLU()(x)\n",
    "  \n",
    "  x = layers.Dropout(0.5)(x)\n",
    "  x = layers.Dense(2)(x)\n",
    "  predictions = layers.Softmax()(x) # predictions = layers.Sigmoid()(x)\n",
    "\n",
    "  model = keras.Model(inputs, predictions, name=model_name)\n",
    "  # Fine tuning requires a lower learning rate. The pre-trained model will be upset by the new rookie layers otherwise.\n",
    "  model.compile( optimizer=tf.keras.optimizers.Adam(0.00001),\n",
    "                 loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                 metrics=['accuracy'] )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnHyoHj8mYiE"
   },
   "outputs": [],
   "source": [
    "transfer_learning_model = transfer_learning_model(input_shape, transfer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tk3F6aiXmiEF"
   },
   "outputs": [],
   "source": [
    "# Check that the output is as expected\n",
    "transfer_learning_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLMi72Pzm6ty"
   },
   "outputs": [],
   "source": [
    "# Use the same callbacks, but with a different model_context\n",
    "model_context = 'transfer_learning'\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join( save_folder, '{}-best_val_loss.h5'.format(model_context) ),\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "# If the validation loss doesn't improve for 20 epochs, stop training\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# If the validation loss doesn't improve for 5 epochs, reduce the learning rate to 0.2 times it's previous value\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsuMzHhmnEl0"
   },
   "outputs": [],
   "source": [
    "# Warm up for 10 epochs\n",
    "n_epochs_warmup=10\n",
    "# Followed by 40 epochs with all params trainable\n",
    "n_epochs_fullblast=40\n",
    "\n",
    "print('Warming up for {} epochs...'.format(n_epochs_warmup))\n",
    "transfer_learning_model.fit(train_gen,\n",
    "          epochs=n_epochs_warmup,\n",
    "          steps_per_epoch=train_gen.n // bs,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_gen.n // bs,\n",
    "          callbacks=[model_checkpoint, earlystopping, reduce_lr])\n",
    "\n",
    "\n",
    "print('Done. Unfreezing all layers and training for {} more epochs...'.format(n_epochs_fullblast))\n",
    "# After the warm-up, unfreeze all the layers of the base ResNet50\n",
    "for layer in transfer_model.layers:\n",
    "  layer.trainable = True\n",
    "\n",
    "transfer_learning_model.fit(train_gen,\n",
    "          epochs=n_epochs_fullblast,\n",
    "          steps_per_epoch=train_gen.n // bs,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_gen.n // bs,\n",
    "          callbacks=[model_checkpoint, earlystopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRz_5Gw5qYd-"
   },
   "outputs": [],
   "source": [
    "# Let's try again with the new transfer-learned model\n",
    "Image(filename=test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NUOLrooCTVC"
   },
   "outputs": [],
   "source": [
    "print( 'model prediction: {}'.format(run_image_on_model(test_img_path, transfer_learning_model, label_map)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmilJUUL8rN8"
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "# Transfer learning and fine-tuning are good options to explore if you want to improve your model's performance. You need to\n",
    "# be mentally prepared to carefully tune learning rates and warm-up procedures though."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImageClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
